{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hgboost_classification_examples.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ZKIel8sR_t",
        "colab_type": "text"
      },
      "source": [
        "hgboost is short for **Hyperoptimized Gradient Boosting**. The aim of hgboost is too determine the most robust model by efficiently searching across the parameter space using hyperoptimization for which the loss is evaluated by means of a train/test-set with k-fold cross-validation. The final optimized model is evaluated on an independent validation set. The incorporated boosting methods are *xgboost*, *catboost* and *lightboost*. hgboost can be applied for classification and regression tasks. This notebook will show some **classification** examples.\n",
        "\n",
        "More information can be found here:\n",
        "\n",
        "* [Github](https://github.com/erdogant/hgboost/blob/master/README.md)\n",
        "* [API documentation](https://erdogant.github.io/hgboost/)\n",
        "* [Regression examples Colab](https://colab.research.google.com/github/erdogant/hgboost/blob/master/notebooks/hgboost_regression_examples.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tMFZEZXsSrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABKzMyoMia9i",
        "colab_type": "text"
      },
      "source": [
        "Lets check the version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7_gxhf8t0JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hgboost\n",
        "print(hgboost.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmRLPpcat3V1",
        "colab_type": "text"
      },
      "source": [
        "Import the hgboost library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IOnt3cvUiyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hgboost import hgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wABxqhb8if3R",
        "colab_type": "text"
      },
      "source": [
        "Initialize using specified parameters. The parameters here are the default parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zennDxyBU627",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hgb = hgboost(max_eval=250, threshold=0.5, cv=5, test_size=0.2, val_size=0.2, top_cv_evals=10, random_state=None, verbose=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXqS2mJmitMq",
        "colab_type": "text"
      },
      "source": [
        "Import example dataset. In this case it is the titanic dataset. We are going to set survived as our response variable (y). Lets see how good we can predict survived."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06YMCo-5VQUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import data\n",
        "df = hgb.import_example()\n",
        "y = df['Survived'].values\n",
        "del df['Survived']\n",
        "X = hgb.preprocessing(df, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFW3N8iOi9ww",
        "colab_type": "text"
      },
      "source": [
        "At this point we can initizalize which boosting model we want to **fit**. For **classification** there is the *xgboost*, *lightboost* or *xgboost*. In addition it is possible to fit an **ensemble** of all (specified) models. For demonstration we will first fit using *xgboost*. If other boosting methods are desired, simply uncomment. The pos_label is set to 1 because this depicts whether cases did *survived* this is the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj-Y4M8rVG1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit\n",
        "# results = hgb.lightboost(X, y, pos_label=1)\n",
        "# results = hgb.catboost(X, y, pos_label=1)\n",
        "results = hgb.xgboost(X, y, pos_label=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au8iFMuDkM6t",
        "colab_type": "text"
      },
      "source": [
        "Done! Fast and clean! We evaluated 250 sets of parameters using HyperOpt in a cross-validation to determine the most optimal set of parameters for predictions using the specified evaluation metric (default is *auc*). We can now easily predict new samples using the **predict** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyMewFppkxkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the predictor\n",
        "y_pred, y_proba = hgb.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VozUvfVSlAHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4UT0qXxk_MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First 10 elements\n",
        "y_pred[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hybswe8lSwb",
        "colab_type": "text"
      },
      "source": [
        "Lets examine the hyper-parameters. We can plot all the individual parameters examine the density, and how the parameters evolve over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxu7ShPVIIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make some plots\n",
        "hgb.plot_params(figsize=(20,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-cBhyZ2lteS",
        "colab_type": "text"
      },
      "source": [
        "Examine each of the iterations. The top 10 results with cross validation are depicted with blue bars. The green dashed line is hte best model without using CV. The red dashed line is the best model with CV. It can be seen that iterations are available that scored higher then the CV but are not selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp0lhN7DVogD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hgb.plot(figsize=(15,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi7I2U42mz5X",
        "colab_type": "text"
      },
      "source": [
        "We can now deeper dive into the cross validation of the best performing model (red dashed line) by plotting the scores for the CVs. Here we see the results for the 5 crosses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XObgaHm-myzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hgb.plot_cv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ML7Fb0mi0o",
        "colab_type": "text"
      },
      "source": [
        "Plot the best performing tree, and the ranked features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diy2nHNUV9En",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hgb.treeplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFiIu3OZmuHi",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the results on the independent validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcrKhsM8V-Kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "hgb.plot_validation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X_4KsCufrrC",
        "colab_type": "text"
      },
      "source": [
        "Lets see whether we can improve the results using the ensemble method!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLmo56wYfsXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_ensemble = hgb.ensemble(X, y, pos_label=1, methods=['xgb_clf','ctb_clf','lgb_clf'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6stt8cEBgIEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hgb.plot_validation()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}